{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T17:36:50.440239Z",
     "start_time": "2018-01-19T17:36:49.933584Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T19:05:12.642335Z",
     "start_time": "2018-01-19T19:05:12.415836Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total images <= max_seq_len is 200\n",
      "total images <= max_seq_len is 100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import utils\n",
    "import torch \n",
    "from torch.autograd import Variable\n",
    "from Modules import VAE\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "filename = \"/home/wafa/Desktop/study/sketchRNN/aaron_sheep.npz\"\n",
    "load_data = np.load(filename, encoding = 'latin1')\n",
    "train_set = load_data['train'][:200]\n",
    "test_set = load_data['test'][:100]\n",
    "\n",
    "nb_steps = 100\n",
    "feature_len=5\n",
    "batch_size = 100\n",
    "max_seq_len=250\n",
    "\n",
    "train_set = utils.DataLoader(train_set, batch_size)\n",
    "test_set = utils.DataLoader(test_set, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T19:06:08.624442Z",
     "start_time": "2018-01-19T19:06:08.572205Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, strokeSize, batchSize,  Nhe, Nhd, Nz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.Nz = Nz\n",
    "        self.Nhe = Nhe\n",
    "        self.Nhd = Nhd\n",
    "        self.cell = nn.LSTM(strokeSize, Nhe//2, 1, bidirectional=True, batch_first=True)\n",
    "        self.mu = nn.Linear(Nhe, Nz)\n",
    "        self.sigma = nn.Linear(Nhe, Nz)\n",
    "        self.h0 = nn.Linear(Nz, Nhd*2) # returns h0 and c0\n",
    "        #print(Nh)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, cn) = self.cell(x)\n",
    "        hn = Variable(torch.cat((hn.data[0],hn.data[1]),1))\n",
    "        sigma = self.sigma(hn)\n",
    "        mu = self.mu(hn)\n",
    "        s = sigma\n",
    "        sigma = torch.exp( sigma * 0.5)\n",
    "        eps = Variable(torch.randn(self.Nz))\n",
    "        z = mu + sigma * eps\n",
    "        return F.tanh(self.h0(z)), z, mu, s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T19:06:09.439075Z",
     "start_time": "2018-01-19T19:06:09.354648Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, strokeSize, batchSize,  Nhe, Nhd, Nz, Ny):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.Nhe = Nhe\n",
    "        self.Nhd = Nhd\n",
    "        self.cell = nn.LSTM(strokeSize+Nz, Nhd, 1, batch_first=True)\n",
    "        self.y = nn.Linear(Nhd, Ny)\n",
    "    \n",
    "    def forward(self, x, h0, c0):\n",
    "        output, (hn, cn) = self.cell(x, (h0, c0))\n",
    "        #print(\"avant reshape\", output.size())\n",
    "        #print(output.contiguous().view(-1, self.Nh).size())\n",
    "        output = output.contiguous().view(-1, self.Nhd)\n",
    "        #print(\"after\", output.size())\n",
    "        y = self.y(output)\n",
    "        return y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T19:06:09.817448Z",
     "start_time": "2018-01-19T19:06:09.773744Z"
    }
   },
   "outputs": [],
   "source": [
    "class SketchRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, strokeSize, batchSize, Nhe, Nhd, Nz, Ny, max_seq_len):\n",
    "        super(SketchRNN, self).__init__()\n",
    "        self.batchSize = batchSize\n",
    "        self.Nhe = Nhe\n",
    "        self.Nhd = Nhd\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.Nz = Nz\n",
    "        self.encoder = Encoder(strokeSize, batchSize, Nhe, Nhd, Nz)\n",
    "        self.decoder = Decoder(strokeSize, batchSize, Nhe, Nhd, Nz, Ny)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # we don't take the inisiale S0 in the encoder so we do the 1:\n",
    "        h0, z, mu, sigma = self.encoder(x[:, 1:250+1, :])\n",
    "        #here we take S0\n",
    "        new_input = torch.cat((x[:, :250, :], z.view(self.batchSize, 1, self.Nz).expand(self.batchSize, self.max_seq_len, self.Nz)), 2)\n",
    "        y = self.decoder(new_input, h0[:,:self.Nhd], h0[:,self.Nhd:])\n",
    "        return y, mu, sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T19:06:10.448424Z",
     "start_time": "2018-01-19T19:06:10.302871Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Loss():  \n",
    "    \n",
    "    def bi_normal(self, x1, x2, mu1, mu2, s1, s2, rho):\n",
    "\n",
    "        x1 = x1.contiguous().view(-1, 1).expand(x1.size(0), M)\n",
    "        x2 = x2.contiguous().view(-1, 1).expand(x2.size(0), M)\n",
    "        norm1 = x1 - mu1\n",
    "        norm2 = x2 - mu2\n",
    "        z = torch.div(norm1, s1).pow(2) + torch.div(norm2, s2).pow(2) - 2 * torch.div(torch.mul(rho, torch.mul(norm1, norm2)), torch.mul(s1,s2))\n",
    "        coef = torch.exp(-z/(2*(1-rho.pow(2))))\n",
    "        denom = 2 * F.math.pi * s1 * s2 * torch.sqrt(1-rho.pow(2))\n",
    "        #print(denom)\n",
    "        result = torch.div(coef, denom)\n",
    "        return result\n",
    "\n",
    "    def Lr(self, z_pi, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr, z_pen_logits, x1, x2, pen, seq_len):\n",
    "\n",
    "        indices = []\n",
    "        for i in range(len(seq_len)): indices += [a+250*i for a in range(seq_len[i])]\n",
    "        #indices = torch.LongTensor(indices)\n",
    "        ls = self.bi_normal(x1[indices,], x2[indices,], z_mu1[indices,], z_mu2[indices,], z_sigma1[indices,], z_sigma2[indices,], z_corr[indices,])\n",
    "        ls = torch.mul(ls, z_pi[indices,])\n",
    "        ls = torch.sum(ls, 1, keepdim=True)\n",
    "        \n",
    "        ls = torch.log(ls + 1e-6)\n",
    "        #print(torch.min(ls))\n",
    "        #print(torch.max(ls))\n",
    "        ls = - torch.sum(ls) / (len(x1))\n",
    "        #print(ls)\n",
    "    \n",
    "        lp = torch.log(z_pen_logits)\n",
    "        lp = torch.mul(lp, pen)\n",
    "        lp = torch.sum(lp, 1, keepdim=True)\n",
    "        lp = - torch.mean(lp)\n",
    "        #print(lp)\n",
    "\n",
    "        return ls+lp\n",
    "\n",
    "    def Lkl(self, mu, sigma):\n",
    "        \n",
    "        Lkl = -0.5 * torch.mean(1 + sigma - mu.pow(2) - sigma.exp())     \n",
    "        return Lkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T19:07:34.739080Z",
     "start_time": "2018-01-19T19:06:11.093679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 7.559655666351318\n",
      "l_kl 0.007194652687758207\n",
      "lr 7.386233806610107\n",
      "l_kl 0.020798103883862495\n",
      "lr 6.121695041656494\n",
      "l_kl 0.03388441726565361\n",
      "lr 6.3583831787109375\n",
      "l_kl 0.04627936705946922\n",
      "lr 6.001687049865723\n",
      "l_kl 0.07441698759794235\n",
      "lr 5.0329461097717285\n",
      "l_kl 0.11225830763578415\n",
      "lr 5.055444717407227\n",
      "l_kl 0.13812413811683655\n",
      "lr 4.9937005043029785\n",
      "l_kl 0.1605917066335678\n",
      "lr 4.877598762512207\n",
      "l_kl 0.16483911871910095\n",
      "lr 4.651070594787598\n",
      "l_kl 0.19424736499786377\n"
     ]
    }
   ],
   "source": [
    "M = 20\n",
    "sketchRnn = SketchRNN(5, 100, 256, 512, 128, 6*M+3, 250)\n",
    "lr=1e-2\n",
    "optimizer = optim.Adam(sketchRnn.parameters(),lr) \n",
    "los = []\n",
    "lok = []\n",
    "aal = []\n",
    "loss = Loss()\n",
    "for _ in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    _, x, s = train_set.random_batch()\n",
    "    x = Variable(torch.from_numpy(x).type(torch.FloatTensor))\n",
    "    y, mu, sigma = sketchRnn(x)\n",
    "    z_pen_logits = y[:, -3:]\n",
    "    z_pi, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr = torch.chunk(y[:, :-3], 6, dim=1)\n",
    "    z_pi = F.softmax(z_pi)\n",
    "    z_pen_logits = F.softmax(z_pen_logits)\n",
    "    z_sigma1 = torch.exp(z_sigma1)\n",
    "    z_sigma2 = torch.exp(z_sigma2)\n",
    "    z_corr = F.tanh(z_corr)\n",
    "    targets = x[:, 1:250+1, :].contiguous().view(-1, 5)\n",
    "    x1 = targets[:,0]\n",
    "    x2 = targets[:,1]\n",
    "    pen = targets[:,2:]\n",
    "    #lr = loss(z_pi, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr, z_pen_logits, x1, x2, pen, s)\n",
    "    lr = loss.Lr(z_pi, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr, z_pen_logits, x1, x2, pen, s)\n",
    "    print(\"lr\" ,lr.data[0])\n",
    "    L_kl = loss.Lkl(mu, sigma)\n",
    "    print(\"l_kl\" ,L_kl.data[0])\n",
    "        \n",
    "    #print(lr.data[0])\n",
    "    los.append(lr.data[0])\n",
    "    lok.append(L_kl.data[0])\n",
    "    (lr + 0.25 * L_kl).backward()\n",
    "    aal.append(lr + 0.25 * L_kl)\n",
    "    optimizer.step()\n",
    "    #print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T19:11:01.078337Z",
     "start_time": "2018-01-19T19:11:00.173266Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,6))    \n",
    "fig.suptitle(\"Loss LR\",fontsize=15)\n",
    "plt.plot(range(len(los)),los, 'b-')\n",
    "plt.xlabel(\"Batches\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T19:11:03.392735Z",
     "start_time": "2018-01-19T19:11:03.231553Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,6))    \n",
    "fig.suptitle(\"Loss KL\",fontsize=15)\n",
    "plt.plot(range(len(lok)),lok, 'b-')\n",
    "plt.xlabel(\"Batches\", fontsize=12)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
