{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-13T17:40:04.538907Z",
     "start_time": "2018-01-13T17:40:04.232260Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-14T19:23:23.590688Z",
     "start_time": "2018-01-14T19:23:23.435101Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total images <= max_seq_len is 200\n",
      "total images <= max_seq_len is 100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import utils\n",
    "import torch \n",
    "from torch.autograd import Variable\n",
    "from Modules import VAE\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "filename = \"/home/wafa/Desktop/study/sketchRNN/aaron_sheep.npz\"\n",
    "load_data = np.load(filename, encoding = 'latin1')\n",
    "train_set = load_data['train'][:200]\n",
    "test_set = load_data['test'][:100]\n",
    "\n",
    "nb_steps = 10\n",
    "feature_len=5\n",
    "batch_size = 6\n",
    "max_seq_len=250\n",
    "\n",
    "train_set = utils.DataLoader(train_set, batch_size)\n",
    "test_set = utils.DataLoader(test_set, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T04:09:40.930716Z",
     "start_time": "2018-01-15T04:09:40.886733Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, strokeSize, batchSize, Nh, Nz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.Nz = Nz\n",
    "        self.cell = nn.LSTM(strokeSize, Nh//2, 1, bidirectional=True, batch_first=True)\n",
    "        self.mu = nn.Linear(Nh, Nz)\n",
    "        self.sigma = nn.Linear(Nh, Nz)\n",
    "        self.h0 = nn.Linear(Nz, Nh)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, cn) = self.cell(x)\n",
    "        hn = Variable(torch.cat((hn.data[0],hn.data[1]),1))\n",
    "        sigma = self.sigma(hn)\n",
    "        mu = self.mu(hn)\n",
    "        sigma = torch.exp( sigma * 0.5)\n",
    "        eps = Variable(torch.randn(self.Nz))\n",
    "        z = mu + sigma * eps\n",
    "        return F.tanh(self.h0(z)), z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T04:09:41.774757Z",
     "start_time": "2018-01-15T04:09:41.706225Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, strokeSize, batchSize, Nh, Nz, Ny):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.Nh = Nh\n",
    "        self.cell = nn.LSTM(strokeSize+Nz, Nh, 1, batch_first=True)\n",
    "        self.y = nn.Linear(Nh, Ny)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output, (hn, cn) = self.cell(x)\n",
    "        #print(output.size())\n",
    "        #print(output.contiguous().view(-1, self.Nh).size())\n",
    "        output = output.contiguous().view(-1, self.Nh)\n",
    "        y = self.y(output)\n",
    "        return y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T04:09:42.303477Z",
     "start_time": "2018-01-15T04:09:42.239861Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SketchRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, strokeSize, batchSize, Nh, Nz, Ny, max_seq_len):\n",
    "        super(SketchRNN, self).__init__()\n",
    "        self.batchSize = batchSize\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.Nz = Nz\n",
    "        self.encoder = Encoder(strokeSize, batchSize, Nh, Nz)\n",
    "        self.decoder = Decoder(strokeSize, batchSize, Nh, Nz, Ny)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # we don't take the inisiale S0 in the encoder so we do the 1:\n",
    "        h0, z = self.encoder(x[:, 1:250+1, :])\n",
    "        #here we take S0\n",
    "        new_input = torch.cat((x[:, :250, :], z.view(self.batchSize, 1, self.Nz).expand(self.batchSize, self.max_seq_len, self.Nz)), 2)\n",
    "        y = self.decoder(new_input)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T04:09:43.323036Z",
     "start_time": "2018-01-15T04:09:43.036748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1500, 33])\n"
     ]
    }
   ],
   "source": [
    "M = 5\n",
    "sketchRnn = SketchRNN(5, 6, 100, 32, 6*M+3, 250)\n",
    "_, x, s = train_set.random_batch()\n",
    "\n",
    "x = Variable(torch.from_numpy(x).type(torch.FloatTensor))\n",
    "y = sketchRnn(x)\n",
    "print(y.size()) # reshaped\n",
    "\n",
    "# note that we don't have to follow the same order of y in the paper! here we take the first M elements as z_pi, the second M elements as z_mu1 ...\n",
    "# we just have to respect this order in the loss computation, it makes it easier to user only pytorch and avoid numpy\n",
    "z_pen_logits = y[:, -3:]\n",
    "z_pi, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr = torch.chunk(y[:, :-3], 6, dim=1)\n",
    "z_pi = F.sigmoid(z_pi)\n",
    "z_pen_logits = F.sigmoid(z_pen_logits)\n",
    "z_sigma1 = torch.exp(z_sigma1)\n",
    "z_sigma2 = torch.exp(z_sigma2)\n",
    "z_corr = F.tanh(z_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T04:09:46.300810Z",
     "start_time": "2018-01-15T04:09:46.285668Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bi_normal(x1, x2, mu1, mu2, s1, s2, rho):\n",
    "    \n",
    "    norm1 = x1 - mu1\n",
    "    norm2 = x2 - mu2\n",
    "    z = torch.div(norm1, s1).pow(2) + torch.div(norm2, s2).pow(2) - 2 * torch.div(torch.mul(rho, torch.mul(norm1, norm2)), torch.mul(s1,s2))\n",
    "    coef = torch.exp(-z/(2*(1-rho.pow(2))))\n",
    "    denom = 2 * F.math.pi * s1 * s2 * torch.sqrt(1-rho.pow(2))\n",
    "    result = torch.div(coef, denom)\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T04:12:26.910348Z",
     "start_time": "2018-01-15T04:12:26.874219Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(z_pi, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr, z_pen_logits, x1, x2, pen):\n",
    "    \n",
    "    #TODO optimize the loop\n",
    "    ls = bi_normal(x1, x2, z_mu1[:, 0], z_mu2[:, 0], z_sigma1[:, 0], z_sigma2[:, 0], z_corr[:, 0]).view(1500,1)\n",
    "    for i in range(1, M):\n",
    "        ls2 = bi_normal(x1, x2, z_mu1[:, i], z_mu2[:, i], z_sigma1[:, i], z_sigma2[:, i], z_corr[:, i]).view(1500,1)\n",
    "        ls = torch.cat((ls,ls2), 1)\n",
    "    ls = torch.mul(ls, z_pi)\n",
    "    ls = torch.sum(ls, 1, keepdim=True) # TODO change to Ns not Nmax\n",
    "    ls = - torch.log(ls + 1e-6)\n",
    "    \n",
    "    lp = torch.log(z_pen_logits)\n",
    "    lp = torch.mul(lp, pen)\n",
    "    lp = - torch.sum(lp, 1, keepdim=True)\n",
    "    \n",
    "    return ls+lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T04:12:44.098685Z",
     "start_time": "2018-01-15T04:12:44.071660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 7.7472\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = target[:,0]\n",
    "x2 = target[:,1]\n",
    "pen = target[:,2:]\n",
    "lr = loss(z_pi, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr, z_pen_logits, x1, x2, pen)\n",
    "lr = torch.mean(lr)\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-14T19:24:14.274882Z",
     "start_time": "2018-01-14T19:24:14.067933Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#OLD\n",
    "_, x, s = train_set.random_batch()\n",
    "x = Variable(torch.from_numpy(x).type(torch.FloatTensor))\n",
    "_, (h, c) = encoder(x)\n",
    "h = torch.cat((h.data[0],h.data[1]),1)\n",
    "h0, z = vae(Variable(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-14T20:05:50.252611Z",
     "start_time": "2018-01-14T20:05:50.236704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 251, 133])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x, z.view(6, 1, 128).expand(6, 251, 128)), 2)\n",
    "torch.cat((x, z.view(batch_size, 1, N_z).expand(batch_size, max_seq_len+1, N_z)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-14T19:24:08.513619Z",
     "start_time": "2018-01-14T19:24:08.481610Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(VAE, self).__init__()\n",
    "        self.outputSize = outputSize\n",
    "        self.mu = nn.Linear(inputSize, outputSize)\n",
    "        self.sigma = nn.Linear(inputSize, outputSize)\n",
    "        self.h0 = nn.Linear(outputSize, inputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        sigma = self.sigma(x)\n",
    "        mu = self.mu(x)\n",
    "        sigma = torch.exp( sigma * 0.5)\n",
    "        eps = torch.randn(self.outputSize)\n",
    "        eps = Variable(eps)\n",
    "        z = mu + sigma * eps\n",
    "        return F.tanh( self.h0(z)), z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-13T17:22:08.374670Z",
     "start_time": "2018-01-13T17:22:07.938203Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load('/home/wafa/Desktop/study/sketchRNN/aaron_sheep/train.npy' ,encoding = 'latin1')\n",
    "def pad_batch(batch):\n",
    "    \"\"\"Pad the batch to be stroke-5 bigger format as described in paper.\"\"\"\n",
    "    result = np.zeros((6, 251, 5), dtype=float)\n",
    "    for i in range(6):\n",
    "        l = len(batch[i])\n",
    "        result[i, 0:l, 0:2] = batch[i][:, 0:2]\n",
    "        result[i, 0:l, 3] = batch[i][:, 2]\n",
    "        result[i, 0:l, 2] = 1 - result[i, 0:l, 3]\n",
    "        result[i, l:, 4] = 1\n",
    "        # put in the first token, as described in sketch-rnn methodology\n",
    "        result[i, 1:, :] = result[i, :-1, :]\n",
    "        result[i, 0, :] = 0\n",
    "        result[i, 0, 2] = 1 # setting S_0 from paper.\n",
    "        result[i, 0, 3] = 0\n",
    "        result[i, 0, 4] = 0\n",
    "    return result\n",
    "\n",
    "rnn = nn.LSTM(5, 251, 1)\n",
    "nb=0\n",
    "while nb+6 < 101:\n",
    "    input = Variable(torch.from_numpy(pad_batch(data[nb:nb+6])).type(torch.FloatTensor))\n",
    "    output, hn = rnn(input) \n",
    "    print(output.size())\n",
    "    nb += 6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
